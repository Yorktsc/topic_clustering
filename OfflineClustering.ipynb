{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import scipy as sp\n",
    "import nltk\n",
    "import jieba #中文分词\n",
    "import time\n",
    "import numpy.linalg as LA\n",
    "import queue\n",
    "import csv\n",
    "import re\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/27/ncssc1kj6nqgt3phh_0cfnn40000gn/T/jieba.cache\n",
      "Loading model cost 0.641 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ea37985151ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjieba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_userdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/tony/Desktop/OfflineClustering/RawData/dict-v24.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/jieba/__init__.py\u001b[0m in \u001b[0;36mload_userdict\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m                 \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/jieba/__init__.py\u001b[0m in \u001b[0;36madd_word\u001b[0;34m(self, word, freq, tag)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstrdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         \u001b[0mfreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfreq\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest_freq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFREQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/jieba/__init__.py\u001b[0m in \u001b[0;36msuggest_freq\u001b[0;34m(self, segment, tune)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msegment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mseg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHMM\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m                 \u001b[0mfreq\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFREQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mftotal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0mfreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreq\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFREQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/jieba/__init__.py\u001b[0m in \u001b[0;36mcut\u001b[0;34m(self, sentence, cut_all, HMM)\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mre_han\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcut_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "jieba.load_userdict('/Users/tony/Desktop/OfflineClustering/RawData/dict-v24.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparas\n",
    "DIMENSION = len(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to mess around with JIEBA:\n",
    "1.分词 2.print individual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Tokenizer.cut at 0x1a346d5a20>\n",
      "市场走势\n",
      "Full Mode: \n"
     ]
    }
   ],
   "source": [
    "seg_list = jieba.cut(\"市场走势\")\n",
    "print(seg_list)\n",
    "for token in seg_list:\n",
    "    print(token)\n",
    "print(\"Full Mode: \" + \"/ \".join(seg_list))  # 全模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/27/ncssc1kj6nqgt3phh_0cfnn40000gn/T/jieba.cache\n",
      "Loading model cost 0.769 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title\n",
      "生意///社///：///本周///纯碱///市场走势///下行///（///12.11///-///12.15///）\n",
      "阎庆民///到任///证监会/// ///分管///上市///部///、///投保///部///和///打非///局等/// \n",
      "李克强///：///希望///香港///努力///将///粤港澳///大湾///区///打///造成///国际///经济///竞争///格局///中///的///世界///一流///湾区\n",
      "美国///税改///再遇///阻力/// ///两名///共和党///参议员///一///反对///一///犹疑\n",
      "超///百家///企业///IPO///终止///审查/// ///理性分析///无需///恐慌\n",
      "苏宁///减持///阿里///背后///：///备战///“///新///零售///”///下半场\n",
      "美///零售///数据///强劲///提振///美元/// ///黄金///承压///震荡///走低\n",
      "融资///余额///上涨///23.19///亿元/// /// ///融资///客抢筹///这///20///股///（///附///名单///）\n",
      "黄金///日内///交易///分析///：///金价///围绕///50///日///均线///徘徊/// ///继续///下跌\n",
      "生意///社///：///本周///磷矿石///市场///整体///持稳///，///局部///震荡///（///12.11///-///12.15///）\n",
      "Processed 11 lines.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('/Users/tony/Desktop/OfflineClustering/RawData/datap_news_html.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',') ############分词符号是 ,\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0:\n",
    "            #print(f'Column names are {\", \".join(row)}')\n",
    "            line_count += 1\n",
    "        else:\n",
    "            #print(f'\\t{row[0]} works in the {row[1]} department, and was born in {row[2]}.')\n",
    "            line_count += 1\n",
    "        a = jieba.cut(row[2])\n",
    "        print(\"///\".join(a))\n",
    "        \n",
    "        if line_count > 10:\n",
    "                  break\n",
    "    print(f'Processed {line_count} lines.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = []\n",
    "with open('/Users/tony/Desktop/OfflineClustering/RawData/dict-v24.txt') as file:\n",
    "    reader = csv.reader(file, delimiter = ' ')\n",
    "    for row in reader:\n",
    "        word_list.append(row[0])\n",
    "\n",
    "word_set = set(word_list) \n",
    "\n",
    "word_id_dict = {}\n",
    "for i in range(len(word_list)):\n",
    "    word_id_dict[word_list[i]] = i "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Form the Key Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = '，：/？“”【】 ！。.\\\\*（）()’‘·、!,;:?\"\\''\n",
    "def remove_punctuation(text):\n",
    "    text = re.sub(r'[{}]+'.format(punctuation),'',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {0:\"零\", 1:\"一\",2:\"二\",3:\"三\",4:\"四\",5:\"五\",6:\"六\",7:\"七\",8:\"八\",9:\"九\"}\n",
    "target = [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"0\"]\n",
    "def cleandigit(text):\n",
    "    ret = \"\"\n",
    "    for i in range(len(text)):\n",
    "        if text[i] in target:\n",
    "            ret += dic[int(text[i])]\n",
    "        else:\n",
    "            ret += text[i]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the News Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1001 lines.\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "with open('/Users/tony/Desktop/OfflineClustering/RawData/datap_news_html.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',') ##################### news must be separated by commas\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        line_count += 1\n",
    "        #去除标点符号 改变数字形式\n",
    "        tempstr = remove_punctuation(row[2])\n",
    "        a = jieba.cut(cleandigit(tempstr))\n",
    "        temp = []\n",
    "        for token in a:\n",
    "            temp.append(token)\n",
    "        data.append(temp)\n",
    "        \n",
    "        if line_count > 1000: ############################### decide how many docs you wanna have experiments with\n",
    "                  break\n",
    "    print(f'Processed {line_count} lines.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['阎庆民', '到任', '证监会', '分管', '上市', '部', '投保', '部和', '打非', '局等']\n",
      "15:15:70\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(data[1])\n",
    "data = data[1:]\n",
    "print(word_list[len(word_list) - 1])\n",
    "print('一' in word_id_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Dictionary demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429.17013000000003\n"
     ]
    }
   ],
   "source": [
    "word2vec = {}\n",
    "t = time.process_time()\n",
    "\n",
    "with open(\"/Users/tony/Desktop/OfflineClustering/RawData/dict-v24.txt\") as a, open(\"/Users/tony/Desktop/OfflineClustering/RawData/word2vec-v24.txt\") as b:\n",
    "    keys  = csv.reader(a, delimiter = ' ')\n",
    "    values = csv.reader(b, delimiter = ' ')\n",
    "    line_count = 0\n",
    "    for key,val in zip(keys,values):\n",
    "        word2vec[key[0]] = val[1:]\n",
    "        line_count += 1\n",
    "elapsed_time = time.process_time() - t\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words(BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BOWwithSVD(data, method, dim = 300):#Using global word_set\n",
    "    #Methods: 1.hash, hash to a fixed size; 2.SVD, pick out principal components\n",
    "    doc_vecs = []\n",
    "    if method == \"hash\": #only using mod right now\n",
    "        for document in data:\n",
    "            doc_vec = np.zeros(dim)\n",
    "            for token in document:\n",
    "                if token in word_id_dict.keys():\n",
    "                    pos = word_id_dict[token] \n",
    "                    pos = pos % dim\n",
    "                    doc_vec[pos] += 1\n",
    "            doc_vecs.append(doc_vec)\n",
    "    elif method == \"SVD\":\n",
    "        for document in data:\n",
    "            doc_vec = np.zeros(DIMENSION)\n",
    "            for token in document:\n",
    "                if token in word_id_dict.keys():\n",
    "                    pos = word_id_dict[token] \n",
    "                    doc_vec[pos] += 1\n",
    "            doc_vecs.append(doc_vec)\n",
    "        pca = PCA(n_components=dim)\n",
    "        doc_vecs = np.array(doc_vecs)\n",
    "        doc_vecs = pca.fit(doc_vecs.T).T\n",
    "    elif method == \"naive\":\n",
    "        for document in data:\n",
    "            doc_vec = np.zeros(DIMENSION)\n",
    "            for token in document:\n",
    "                if token in word_id_dict.keys():\n",
    "                    pos = word_id_dict[token] \n",
    "                    doc_vec[pos] += 1\n",
    "            doc_vecs.append(doc_vec)\n",
    "    return doc_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "995\n"
     ]
    }
   ],
   "source": [
    "data = data[1:]\n",
    "doc_vecs = BOWwithSVD(data, method = \"naive\")\n",
    "print(len(doc_vecs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_similarity(doc1, doc2):\n",
    "    if len(doc1) != len(doc2):\n",
    "        return 0\n",
    "    else:\n",
    "        nom = np.dot(doc1, doc2)\n",
    "        denom = LA.norm(doc1) + LA.norm(doc2)\n",
    "        return nom/denom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TF_IDF(word, doc, corpus):\n",
    "    count = 0\n",
    "    for w in doc:\n",
    "        if word == w:\n",
    "            count += 1\n",
    "    count2 = 0\n",
    "    for d in corpus:\n",
    "        if word in d:\n",
    "            count2 += 1\n",
    "    TF = count / len(doc)\n",
    "    IDF = np.log(len(corpus) / (count2 + 1))\n",
    "    return TF*IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bisectingkmeans(doc_vecs, k): #using global word2vec dictionary loaded above\n",
    "    clusters = [[doc_vecs]]\n",
    "    \n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 'a')\n"
     ]
    }
   ],
   "source": [
    "a = queue.PriorityQueue()\n",
    "a.put((100,'a'))\n",
    "a.put((1000,'b'))\n",
    "print(a.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
